{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ôüÔ∏è Chess AI ‚Äî Projet M1\n",
    "### Intelligence Artificielle pour les √âchecs : Ouvertures + Minimax + Q-Learning\n",
    "\n",
    "---\n",
    "\n",
    "**Auteurs :** [Vos noms]  \n",
    "**Encadrant :** [Nom de l'encadrant]  \n",
    "**Universit√© :** [Universit√©]  \n",
    "**Ann√©e :** 2024-2025\n",
    "\n",
    "---\n",
    "\n",
    "## Plan du Notebook\n",
    "\n",
    "1. [Installation & Imports](#1)\n",
    "2. [Gestion des r√®gles d'√©checs (python-chess)](#2)\n",
    "3. [Livre d'ouvertures](#3)\n",
    "4. [√âvaluateur de position](#4)\n",
    "5. [Minimax + Alpha-B√™ta](#5)\n",
    "6. [Q-Learning & Self-Play](#6)\n",
    "7. [Agent Hybride](#7)\n",
    "8. [√âvaluation & Benchmarks](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Installation & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances\n",
    "!pip install python-chess matplotlib numpy ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import chess\n",
    "import chess.svg\n",
    "import chess.pgn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from IPython.display import SVG, display, HTML\n",
    "\n",
    "# Modules du projet\n",
    "from src.engine.board import ChessBoard\n",
    "from src.engine.evaluator import Evaluator\n",
    "from src.engine.minimax import MinimaxAgent\n",
    "from src.opening.opening_book import OpeningBook\n",
    "from src.rl.q_learning import QLearningAgent\n",
    "from src.agent import ChessAI\n",
    "from src.utils.visualization import (\n",
    "    render_board, plot_eval_curve,\n",
    "    plot_training_progress, plot_piece_heatmap\n",
    ")\n",
    "\n",
    "print('‚úÖ Tous les modules charg√©s avec succ√®s !')\n",
    "print(f'python-chess version : {chess.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Gestion des R√®gles d'√âchecs avec `python-chess`\n",
    "\n",
    "La librairie `python-chess` g√®re int√©gralement :\n",
    "- ‚úÖ Les mouvements l√©gaux (toutes pi√®ces)\n",
    "- ‚úÖ √âchec, mat, pat, roque, prise en passant\n",
    "- ‚úÖ Notation FEN et PGN\n",
    "- ‚úÖ Hash Zobrist pour la table de transposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation d'un √©chiquier initial\n",
    "cb = ChessBoard()\n",
    "print('Position initiale (FEN) :')\n",
    "print(cb.to_fen())\n",
    "print()\n",
    "print(f'Nombre de coups l√©gaux : {len(cb.get_legal_moves())}')\n",
    "print(f'Coups l√©gaux (UCI) : {cb.get_legal_moves_uci()[:5]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de l'√©chiquier en SVG\n",
    "display(render_board(cb.board, size=350))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test des r√®gles sp√©ciales\n",
    "# Exemple de Ruy Lopez (3 premiers coups)\n",
    "cb = ChessBoard()\n",
    "moves = ['e2e4', 'e7e5', 'g1f3', 'b8c6', 'f1b5']\n",
    "for uci in moves:\n",
    "    success = cb.push_uci(uci)\n",
    "    print(f'Coup {uci}: {\"‚úÖ\" if success else \"‚ùå\"}')\n",
    "\n",
    "print(f'\\nPosition apr√®s 1.e4 e5 2.Nf3 Nc6 3.Bb5 (Ruy Lopez) :')\n",
    "display(render_board(cb.board, size=350, last_move=chess.Move.from_uci('f1b5')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : Position de mat en 1 (Scholar's Mate)\n",
    "scholars_mate_fen = 'r1bqkb1r/pppp1Qpp/2n2n2/4p3/2B1P3/8/PPPP1PPP/RNB1K1NR b KQkq -'\n",
    "cb_mate = ChessBoard(fen=scholars_mate_fen)\n",
    "print(f'√âchec et mat : {cb_mate.is_checkmate()}')\n",
    "print(f'R√©sultat : {cb_mate.get_result()}')\n",
    "display(render_board(cb_mate.board, size=350))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Livre d'Ouvertures\n",
    "\n",
    "L'agent consulte d'abord une base d'ouvertures classiques avant de calculer.\n",
    "\n",
    "**Ouvertures incluses :** Ruy Lopez ¬∑ Sicilienne ¬∑ Fran√ßaise ¬∑ Gambit Dame ¬∑ Italienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du livre d'ouvertures\n",
    "book = OpeningBook(random_weight=True, max_opening_plies=20)\n",
    "\n",
    "# Test sur la position initiale\n",
    "board = chess.Board()\n",
    "print('=== Test du livre d\\'ouvertures ===')\n",
    "print(f'Position : {board.fen()[:30]}...')\n",
    "\n",
    "# Jouer 5 coups via le livre\n",
    "for i in range(5):\n",
    "    move = book.get_move(board)\n",
    "    if move:\n",
    "        opening_name = book.get_opening_name(board)\n",
    "        print(f'  Coup {i+1}: {move.uci()} ‚Äî {opening_name}')\n",
    "        board.push(move)\n",
    "    else:\n",
    "        print(f'  Coup {i+1}: Hors du livre')\n",
    "        break\n",
    "\n",
    "print(f'\\nPosition finale :')\n",
    "display(render_board(board, size=350, last_move=move))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier l'ouverture apr√®s quelques coups\n",
    "test_positions = [\n",
    "    (['e2e4', 'c7c5'], 'Sicilienne ?'),\n",
    "    (['e2e4', 'e7e6'], 'Fran√ßaise ?'),\n",
    "    (['d2d4', 'd7d5', 'c2c4'], 'Gambit Dame ?'),\n",
    "    (['e2e4', 'e7e5', 'g1f3', 'b8c6', 'f1b5'], 'Ruy Lopez ?'),\n",
    "]\n",
    "\n",
    "for moves, expected in test_positions:\n",
    "    b = chess.Board()\n",
    "    for m in moves:\n",
    "        b.push(chess.Move.from_uci(m))\n",
    "    name = book.get_opening_name(b)\n",
    "    print(f'  {expected:<25} ‚Üí {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. √âvaluateur de Position\n",
    "\n",
    "La fonction d'√©valuation heuristique combine :\n",
    "- **Mat√©riel** : valeur des pi√®ces (Pion=100, Cavalier=320, ...)\n",
    "- **Tables positionnelles** (PST) : bonus/p√©nalit√©s selon la case\n",
    "- **Mobilit√©** : bonus pour les coups disponibles\n",
    "- **Contr√¥le du centre** : bonus pour les cases centrales\n",
    "- **S√©curit√© du roi** : p√©nalit√© si le roi est expos√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "\n",
    "# √âvaluation de la position initiale\n",
    "board = chess.Board()\n",
    "score = evaluator.evaluate(board)\n",
    "print(f'Position initiale : {score:.1f} centipawns')\n",
    "\n",
    "# Apr√®s 1.e4\n",
    "board.push(chess.Move.from_uci('e2e4'))\n",
    "score = evaluator.evaluate(board)\n",
    "print(f'Apr√®s 1.e4 : {score:.1f} centipawns')\n",
    "\n",
    "# Avantage mat√©riel\n",
    "board_advantage = chess.Board('rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKB1R w KQkq -')\n",
    "score = evaluator.evaluate(board_advantage)\n",
    "print(f'Blancs avec Cavalier en moins : {score:.1f} centipawns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbe d'√©valuation sur une partie exemple\n",
    "board = chess.Board()\n",
    "sample_moves = [\n",
    "    'e2e4','e7e5','g1f3','b8c6','f1b5','a7a6','b5a4','g8f6',\n",
    "    'e1g1','f8e7','f1e1','b7b5','a4b3','d7d6','c2c3','e8g8',\n",
    "]\n",
    "\n",
    "eval_scores = []\n",
    "move_labels = []\n",
    "\n",
    "for uci in sample_moves:\n",
    "    move = chess.Move.from_uci(uci)\n",
    "    board.push(move)\n",
    "    eval_scores.append(evaluator.evaluate(board))\n",
    "    move_labels.append(uci)\n",
    "\n",
    "fig = plot_eval_curve(eval_scores, move_labels, \n",
    "                      title=\"Courbe d'√©valuation ‚Äî Ruy Lopez (Morphy)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. Minimax + Alpha-B√™ta Pruning\n",
    "\n",
    "### Principe de l'algorithme\n",
    "\n",
    "```\n",
    "                    N≈ìud racine (Blancs MAX)\n",
    "                   /           |          \\\n",
    "              e2e4           d2d4         c2c4\n",
    "             (MIN)          (MIN)        (MIN)\n",
    "            /    \\         /    \\\n",
    "          e7e5  c7c5    d7d5   g8f6\n",
    "          ...   ...    ...    ...\n",
    "```\n",
    "\n",
    "L'√©lagage Alpha-B√™ta √©vite d'explorer des branches qui ne peuvent pas influencer le r√©sultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du Minimax\n",
    "minimax = MinimaxAgent(depth=3, time_limit=10.0)\n",
    "board = chess.Board()\n",
    "\n",
    "# Position apr√®s 1.e4 e5\n",
    "board.push(chess.Move.from_uci('e2e4'))\n",
    "board.push(chess.Move.from_uci('e7e5'))\n",
    "\n",
    "print('Position analys√©e :')\n",
    "display(render_board(board, size=300))\n",
    "\n",
    "print('\\n[Minimax profondeur 3 ‚Äî blancs √† jouer]')\n",
    "start = time.time()\n",
    "best_move = minimax.choose_move(board)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f'Meilleur coup : {best_move.uci()}')\n",
    "print(f'N≈ìuds visit√©s : {minimax.nodes_visited:,}')\n",
    "print(f'Temps : {elapsed:.3f}s')\n",
    "\n",
    "display(render_board(board, size=300, \n",
    "                     last_move=best_move,\n",
    "                     arrows=[(best_move.from_square, best_move.to_square, '#00ff00')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des profondeurs (vitesse vs qualit√©)\n",
    "depths = [1, 2, 3, 4]\n",
    "results = []\n",
    "\n",
    "board = chess.Board()\n",
    "for _ in range(4):  # Position apr√®s 4 coups\n",
    "    board.push(board.legal_moves.__iter__().__next__())\n",
    "\n",
    "print('‚è±Ô∏è Benchmark Minimax selon la profondeur :')\n",
    "print(f'{\"Profondeur\":>12} | {\"N≈ìuds\":>10} | {\"Temps\":>8} | {\"Coup\":>6}')\n",
    "print('-' * 50)\n",
    "\n",
    "for d in depths:\n",
    "    agent = MinimaxAgent(depth=d)\n",
    "    start = time.time()\n",
    "    move = agent.choose_move(board)\n",
    "    elapsed = time.time() - start\n",
    "    results.append({'depth': d, 'nodes': agent.nodes_visited, 'time': elapsed})\n",
    "    print(f'{d:>12} | {agent.nodes_visited:>10,} | {elapsed:>7.3f}s | {move.uci()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'impact de l'Alpha-B√™ta\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "depths_list = [r['depth'] for r in results]\n",
    "nodes_list = [r['nodes'] for r in results]\n",
    "times_list = [r['time'] for r in results]\n",
    "\n",
    "ax1.bar(depths_list, nodes_list, color='#2196F3', alpha=0.8)\n",
    "ax1.set_xlabel('Profondeur')\n",
    "ax1.set_ylabel('N≈ìuds visit√©s')\n",
    "ax1.set_title('N≈ìuds explor√©s par profondeur')\n",
    "ax1.set_xticks(depths_list)\n",
    "\n",
    "ax2.bar(depths_list, times_list, color='#FF5722', alpha=0.8)\n",
    "ax2.set_xlabel('Profondeur')\n",
    "ax2.set_ylabel('Temps (secondes)')\n",
    "ax2.set_title('Temps de calcul par profondeur')\n",
    "ax2.set_xticks(depths_list)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. Q-Learning & Self-Play\n",
    "\n",
    "### Formalisation\n",
    "\n",
    "| Composant | D√©finition |\n",
    "|-----------|------------|\n",
    "| **√âtat s** | Hash FEN de la position |\n",
    "| **Action a** | Coup UCI (ex: `e2e4`) |\n",
    "| **R√©compense r** | +1 victoire ¬∑ -1 d√©faite ¬∑ 0 nulle ¬∑ ¬±0.1 capture |\n",
    "| **Politique œÄ** | Œµ-greedy (exploration vs exploitation) |\n",
    "\n",
    "**R√®gle de mise √† jour :**\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'agent Q-Learning\n",
    "rl_agent = QLearningAgent(\n",
    "    alpha=0.3,          # Taux d'apprentissage\n",
    "    gamma=0.95,         # Facteur d'actualisation\n",
    "    epsilon=1.0,        # Exploration initiale : 100%\n",
    "    epsilon_decay=0.995,# D√©croissance par √©pisode\n",
    "    epsilon_min=0.05,   # Exploration minimale : 5%\n",
    ")\n",
    "\n",
    "print('Agent Q-Learning cr√©√© :')\n",
    "print(f'  Œ± (learning rate)  : {rl_agent.alpha}')\n",
    "print(f'  Œ≥ (discount)       : {rl_agent.gamma}')\n",
    "print(f'  Œµ initial          : {rl_agent.epsilon}')\n",
    "print(f'  Œµ min              : {rl_agent.epsilon_min}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement par self-play\n",
    "N_EPISODES = 500  # Augmenter √† 2000-5000 pour de meilleurs r√©sultats\n",
    "\n",
    "# Suivi des m√©triques pour visualisation\n",
    "stats_history = []\n",
    "win_rates = []\n",
    "\n",
    "print(f'üéØ D√©marrage du self-play : {N_EPISODES} √©pisodes')\n",
    "print('‚îÄ' * 60)\n",
    "\n",
    "VERBOSE_EVERY = 50\n",
    "for ep in range(N_EPISODES):\n",
    "    result, _ = rl_agent.self_play_episode(max_moves=150)\n",
    "    \n",
    "    stats = rl_agent.export_stats()\n",
    "    stats_history.append(stats.copy())\n",
    "    \n",
    "    if (ep + 1) % VERBOSE_EVERY == 0:\n",
    "        total = stats['wins'] + stats['draws'] + stats['losses']\n",
    "        wr = stats['wins'] / total * 100 if total else 0\n",
    "        print(f'Ep {ep+1:5d} | Œµ={stats[\"epsilon\"]:.3f} | '\n",
    "              f'V:{stats[\"wins\"]} N:{stats[\"draws\"]} D:{stats[\"losses\"]} | '\n",
    "              f'Q-√©tats: {stats[\"q_table_size\"]:,}')\n",
    "\n",
    "print('‚îÄ' * 60)\n",
    "print(f'‚úÖ Entra√Ænement termin√©. {len(rl_agent.q_table):,} √©tats Q appris.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'entra√Ænement\n",
    "fig = plot_training_progress(stats_history, window=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du mod√®le\n",
    "rl_agent.save('../data/q_table.pkl')\n",
    "print('Q-table sauvegard√©e !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. Agent Hybride (Ouvertures + RL + Minimax)\n",
    "\n",
    "L'agent hybride combine les trois composants :\n",
    "\n",
    "```\n",
    "Position ‚Üí [Ouvertures] ‚Üí coup connu ?\n",
    "                ‚Üì non\n",
    "         [Q-Learning] ‚Üí Q-value significative ?\n",
    "                ‚Üì non\n",
    "          [Minimax]  ‚Üí meilleur coup calcul√©\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation de l'agent hybride\n",
    "ai_white = ChessAI(\n",
    "    mode='hybrid',\n",
    "    minimax_depth=3,\n",
    "    minimax_time_limit=5.0,\n",
    "    q_table_path='../data/q_table.pkl',\n",
    "    color=chess.WHITE,\n",
    ")\n",
    "\n",
    "ai_black = ChessAI(\n",
    "    mode='minimax',  # Agent Minimax pur comme adversaire\n",
    "    minimax_depth=2,\n",
    "    color=chess.BLACK,\n",
    ")\n",
    "\n",
    "print('Agents cr√©√©s :')\n",
    "print(f'  {ai_white}')\n",
    "print(f'  {ai_black}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouer une partie compl√®te\n",
    "print('‚ôüÔ∏è Partie : Agent Hybride (Blancs) vs Minimax (Noirs)')\n",
    "print('=' * 55)\n",
    "\n",
    "game_result = ai_white.play_game(opponent=ai_black, max_moves=80, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le PGN de la partie\n",
    "print('PGN de la partie :')\n",
    "print(game_result['pgn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser les sources de d√©cision\n",
    "log = game_result['log']\n",
    "sources = {}\n",
    "for entry in log:\n",
    "    src = entry['source']\n",
    "    sources[src] = sources.get(src, 0) + 1\n",
    "\n",
    "print('Sources de d√©cision :')\n",
    "for src, count in sorted(sources.items()):\n",
    "    pct = count / len(log) * 100\n",
    "    bar = '‚ñà' * int(pct / 2)\n",
    "    print(f'  {src:<15} : {count:3d} coups ({pct:5.1f}%) {bar}')\n",
    "\n",
    "# Pie chart\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "colors = {'opening_book': '#4CAF50', 'q_learning': '#2196F3', 'minimax': '#FF5722'}\n",
    "ax.pie(\n",
    "    list(sources.values()),\n",
    "    labels=list(sources.keys()),\n",
    "    colors=[colors.get(k, '#9E9E9E') for k in sources.keys()],\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90\n",
    ")\n",
    "ax.set_title('Distribution des sources de d√©cision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8. √âvaluation & Benchmarks\n",
    "\n",
    "### M√©triques d'√©valuation\n",
    "- **Taux de victoire** sur N parties contre un adversaire fixe\n",
    "- **Profondeur effective** (n≈ìuds explor√©s par seconde)\n",
    "- **Taille de la Q-table** (√©tats couverts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark : N parties Minimax vs Minimax (profondeurs diff√©rentes)\n",
    "N_GAMES = 10  # Augmenter pour un benchmark complet\n",
    "\n",
    "print(f'üèÜ Tournoi interne ({N_GAMES} parties par configuration)')\n",
    "print('=' * 60)\n",
    "\n",
    "configs = [\n",
    "    ('Minimax-d2 vs Minimax-d3', 2, 3),\n",
    "    ('Minimax-d3 vs Minimax-d3', 3, 3),\n",
    "]\n",
    "\n",
    "tournament_results = []\n",
    "\n",
    "for name, depth_w, depth_b in configs:\n",
    "    wins, draws, losses = 0, 0, 0\n",
    "    for g in range(N_GAMES):\n",
    "        white = ChessAI(mode='minimax', minimax_depth=depth_w,\n",
    "                        minimax_time_limit=3.0, color=chess.WHITE)\n",
    "        black = ChessAI(mode='minimax', minimax_depth=depth_b,\n",
    "                        minimax_time_limit=3.0, color=chess.BLACK)\n",
    "        res = white.play_game(opponent=black, max_moves=60, verbose=False)\n",
    "        if res['result'] == '1-0':   wins += 1\n",
    "        elif res['result'] == '0-1': losses += 1\n",
    "        else:                         draws += 1\n",
    "    \n",
    "    wr = wins / N_GAMES * 100\n",
    "    print(f'{name}: V={wins} N={draws} D={losses} | WR Blancs={wr:.0f}%')\n",
    "    tournament_results.append({'name': name, 'wins': wins, 'draws': draws, 'losses': losses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© des performances RL apr√®s entra√Ænement\n",
    "stats = rl_agent.export_stats()\n",
    "print('üìä Statistiques Q-Learning :')\n",
    "for k, v in stats.items():\n",
    "    print(f'  {k:<20} : {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap des pi√®ces dans la position finale\n",
    "final_board = chess.Board()\n",
    "for uci in game_result['moves']:\n",
    "    try:\n",
    "        final_board.push(chess.Move.from_uci(uci))\n",
    "    except:\n",
    "        break\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Afficher deux heatmaps\n",
    "plt.sca(ax1)\n",
    "plot_piece_heatmap(final_board, chess.WHITE)\n",
    "plt.sca(ax2)\n",
    "plot_piece_heatmap(final_board, chess.BLACK)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "| Composant | Technologie | R√¥le |\n",
    "|-----------|-------------|------|\n",
    "| R√®gles | `python-chess` | Mouvements l√©gaux, FEN/PGN |\n",
    "| Ouvertures | Livre int√©gr√© | Phase d'ouverture |\n",
    "| Milieu de jeu | Minimax + Œ±-Œ≤ | D√©cision principale |\n",
    "| Apprentissage | Q-Learning + Self-play | Am√©lioration continue |\n",
    "\n",
    "**Pistes d'am√©lioration :**\n",
    "- Int√©grer un vrai livre Polyglot (ex: `baron30.bin`)\n",
    "- Remplacer Q-Learning par MCTS (Monte Carlo Tree Search)\n",
    "- Entra√Æner un r√©seau de neurones (DQN ou AlphaZero simplifi√©)\n",
    "- Ajouter la gestion des fins de partie (Endgame tablebases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
