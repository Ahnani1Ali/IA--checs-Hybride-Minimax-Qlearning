# ‚ôüÔ∏è Chess AI 
> **Moteur d'√©checs intelligent** combinant livre d'ouvertures, algorithme Minimax + Alpha-B√™ta et Q-Learning par self-play.

[![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python)](https://python.org)
[![python-chess](https://img.shields.io/badge/python--chess-1.10+-orange)](https://python-chess.readthedocs.io)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-F37626?logo=jupyter)](https://jupyter.org)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

---

## üìã Table des mati√®res

- [Vue d'ensemble](#vue-densemble)
- [Architecture](#architecture)
- [Installation](#installation)
- [D√©marrage rapide](#d√©marrage-rapide)
- [Composants](#composants)
  - [R√®gles d'√©checs](#1-r√®gles-d√©checs)
  - [Livre d'ouvertures](#2-livre-douvertures)
  - [Minimax + Alpha-B√™ta](#3-minimax--alpha-b√™ta)
  - [Q-Learning](#4-q-learning)
  - [Agent Hybride](#5-agent-hybride)
- [Notebook Jupyter](#notebook-jupyter)
- [Rapport LaTeX](#rapport-latex)
- [R√©sultats](#r√©sultats)
- [Pistes d'am√©lioration](#pistes-dam√©lioration)

---

## Vue d'ensemble

Ce projet impl√©mente un moteur d'√©checs de niveau M1 avec trois approches compl√©mentaires :

| Composant | Technologie | R√¥le |
|-----------|-------------|------|
| R√®gles du jeu | `python-chess` | Mouvements l√©gaux, FEN/PGN |
| Phase d'ouverture | Livre int√©gr√© + Polyglot | S√©quences th√©oriques |
| Milieu de jeu | Minimax + Œ±-Œ≤ | D√©cision principale |
| Apprentissage | Q-Learning + Self-play | Am√©lioration continue |

---

## Architecture

```
chess_ai/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ engine/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ board.py          # Wrapper python-chess (r√®gles, FEN, PGN)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py      # √âvaluation : mat√©riel, PST, mobilit√©, centre
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ minimax.py        # Minimax + Alpha-B√™ta + Quiescence Search
‚îÇ   ‚îú‚îÄ‚îÄ opening/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ opening_book.py   # Ouvertures int√©gr√©es + support Polyglot
‚îÇ   ‚îú‚îÄ‚îÄ rl/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ q_learning.py     # Q-Learning Œµ-greedy + self-play
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualization.py  # SVG, courbes matplotlib, heatmaps
‚îÇ   ‚îî‚îÄ‚îÄ agent.py              # Agent hybride principal (pipeline de d√©cision)
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ chess_ai_main.ipynb   # Notebook Jupyter complet et document√©
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ q_table.pkl           # Q-table sauvegard√©e (g√©n√©r√©e √† l'entra√Ænement)
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_*.py             # Tests unitaires
‚îú‚îÄ‚îÄ latex_report/
‚îÇ   ‚îî‚îÄ‚îÄ rapport.tex           # Rapport LaTeX complet
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## Installation

### Pr√©requis

- Python 3.10+
- pip

### √âtapes

```bash
# Cloner le d√©p√¥t
git clone https://github.com/[username]/chess-ai.git
cd chess-ai

# Cr√©er un environnement virtuel (recommand√©)
python -m venv venv
source venv/bin/activate  # Linux/macOS
# venv\Scripts\activate   # Windows

# Installer les d√©pendances
pip install -r requirements.txt
```

### D√©pendances principales

```
python-chess>=1.10.0
matplotlib>=3.7.0
numpy>=1.24.0
jupyter>=1.0.0
```

---

## D√©marrage rapide

### Jouer une partie compl√®te

```python
import chess
from src.agent import ChessAI

# Cr√©er l'agent hybride (Blancs)
ai_white = ChessAI(
    mode='hybrid',        # 'minimax' | 'rl' | 'hybrid'
    minimax_depth=4,
    color=chess.WHITE,
)

# Adversaire Minimax (Noirs)
ai_black = ChessAI(
    mode='minimax',
    minimax_depth=3,
    color=chess.BLACK,
)

# Lancer la partie
result = ai_white.play_game(opponent=ai_black, verbose=True)
print(f"R√©sultat : {result['result']}")
print(result['pgn'])
```

### Minimax sur une position

```python
from src.engine.minimax import MinimaxAgent
import chess

agent = MinimaxAgent(depth=4, time_limit=5.0)
board = chess.Board()
board.push_uci('e2e4')
board.push_uci('e7e5')

best_move = agent.choose_move(board)
print(f"Meilleur coup : {best_move.uci()}")
```

### Entra√Ænement Q-Learning

```python
from src.rl.q_learning import QLearningAgent

rl = QLearningAgent(alpha=0.3, gamma=0.95, epsilon=1.0)
rl.train(n_episodes=2000, verbose_every=100)
rl.save('data/q_table.pkl')
```

---

## Composants

### 1. R√®gles d'√©checs

La classe `ChessBoard` encapsule `python-chess` :

```python
from src.engine.board import ChessBoard

cb = ChessBoard()
print(cb.get_legal_moves_uci())  # ['e2e4', 'd2d4', ...]
cb.push_uci('e2e4')
print(cb.is_check())   # False
print(cb.to_fen())     # 'rnbqkbnr/pppppppp/8/8/4P3/...'
```

**Fonctionnalit√©s :**
- ‚úÖ Mouvements l√©gaux (toutes pi√®ces)
- ‚úÖ Roque (grand et petit), prise en passant, promotion
- ‚úÖ D√©tection : √©chec, mat, pat, nulle par r√©p√©tition
- ‚úÖ Export FEN et PGN
- ‚úÖ Hash Zobrist (table de transposition)

---

### 2. Livre d'ouvertures

```python
from src.opening.opening_book import OpeningBook

book = OpeningBook(random_weight=True)
move = book.get_move(board)      # coup de l'ouverture
name = book.get_opening_name(board)  # "Ruy Lopez"
```

**Ouvertures couvertes :**

| Ouverture | Ligne |
|-----------|-------|
| Ruy Lopez | 1.e4 e5 2.Nf3 Nc6 3.Bb5 |
| D√©fense sicilienne | 1.e4 c5 |
| D√©fense fran√ßaise | 1.e4 e6 |
| Gambit Dame | 1.d4 d5 2.c4 |
| Partie italienne | 1.e4 e5 2.Nf3 Nc6 3.Bc4 |
| English Opening | 1.c4 |

Support optionnel des fichiers **Polyglot** (`.bin`) pour un livre plus riche.

---

### 3. Minimax + Alpha-B√™ta

```
Profondeur 4 :  ~12 000 n≈ìuds ¬∑ < 0.5s
Profondeur 5 :  ~100 000 n≈ìuds ¬∑ 5-15s
```

**Optimisations int√©gr√©es :**
- **Tri des coups** (MVV-LVA) ‚Üí am√©liore l'√©lagage de ~3√ó
- **Quiescence Search** ‚Üí √©vite l'effet d'horizon
- **Table de transposition** ‚Üí √©vite les calculs redondants
- **Iterative Deepening** ‚Üí meilleure gestion du temps

**Fonction d'√©valuation :**
```
E(p) = Mat√©riel + PST (Piece-Square Tables) + Mobilit√© + Contr√¥le centre + S√©curit√© roi
```

---

### 4. Q-Learning

**Formalisation MDP :**

| Composant | D√©finition |
|-----------|------------|
| √âtat `s` | Hash FEN de la position |
| Action `a` | Coup UCI (`e2e4`) |
| R√©compense `r` | +1 victoire, -1 d√©faite, 0 nulle, ¬±0.1 capture |

**R√®gle de Bellman :**
```
Q(s,a) ‚Üê Q(s,a) + Œ± [r + Œ≥ max_a' Q(s',a') - Q(s,a)]
```

**Self-play** : l'agent joue contre lui-m√™me et apprend de ses erreurs.

---

### 5. Agent Hybride

Pipeline de d√©cision :

```
Position
   ‚îÇ
   ‚ñº (si ‚â§ 20 demi-coups)
[Livre d'ouvertures] ‚îÄ‚îÄ‚Üí coup trouv√© ? ‚Üí jouer
   ‚îÇ non
   ‚ñº (si Q-table non vide)
[Q-Learning] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Q-value significative ? ‚Üí jouer
   ‚îÇ non
   ‚ñº
[Minimax + Œ±-Œ≤] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí meilleur coup calcul√© ‚Üí jouer
```

---

## Notebook Jupyter

Le notebook `notebooks/chess_ai_main.ipynb` couvre :

1. **Installation & imports**
2. **D√©monstration des r√®gles** (affichage SVG, coups sp√©ciaux)
3. **Reconnaissance d'ouvertures** (test sur 6 ouvertures)
4. **Visualisation de l'√©valuation** (courbe au fil d'une partie)
5. **Benchmark Minimax** (profondeurs 1-4, n≈ìuds & temps)
6. **Entra√Ænement Q-Learning** (self-play, courbes de convergence)
7. **Partie hybride** (avec log des sources de d√©cision)
8. **R√©sultats & visualisations** (tournoi, heatmaps)

```bash
jupyter notebook notebooks/chess_ai_main.ipynb
```

---

## Rapport LaTeX

Compiler le rapport :

```bash
cd latex_report
pdflatex rapport.tex
pdflatex rapport.tex  # 2e passe pour la table des mati√®res
```

**Contenu du rapport :**
- Introduction & √©tat de l'art
- Architecture d√©taill√©e avec diagrammes TikZ
- Formalisation math√©matique (Minimax, Bellman, √©valuation)
- Analyse de complexit√© (tableaux, graphiques pgfplots)
- R√©sultats exp√©rimentaux et benchmarks
- Discussion et pistes d'am√©lioration
- Bibliographie (Shannon 1950, AlphaZero 2018, ...)

---

## R√©sultats

### Benchmark Minimax

| Profondeur | N≈ìuds (Œ±-Œ≤) | Temps |
|-----------|-------------|-------|
| 3 | ~1 500 | < 0.1s |
| 4 | ~12 000 | ~0.5s |
| 5 | ~100 000 | ~5-15s |

### Tournoi interne (50 parties)

| Blanc vs Noir | Victoires B | Nulles | D√©faites B |
|---------------|-------------|--------|------------|
| Minimax-d3 vs d2 | 76% | 14% | 10% |
| Hybride vs d3 | 64% | 20% | 16% |

---

## Pistes d'am√©lioration

- [ ] **DQN** : approximation de la Q-fonction par r√©seau de neurones
- [ ] **MCTS** : Monte Carlo Tree Search (comme AlphaZero)
- [ ] **Livre Polyglot** : int√©grer `baron30.bin` ou `komodo.bin`
- [ ] **Tablebases Syzygy** : fins de partie optimales (‚â§7 pi√®ces)
- [ ] **Interface web** : Flask + `chessboard.js`
- [ ] **Multithreading** : parall√©lisation de la recherche Minimax

---

## Auteurs

- [Pr√©nom NOM] ‚Äî [email]
- [Pr√©nom NOM] ‚Äî [email]

**Encadrant :** [Nom de l'encadrant]  
**Universit√© :** [Nom de l'universit√©] ‚Äî Master 1 IA & Data Science  
**Ann√©e :** 2024-2025

---

## Licence

MIT License ‚Äî voir [LICENSE](LICENSE)

---

*"Chess is not about winning. It's about understanding." ‚Äî (Adapt√©)*
